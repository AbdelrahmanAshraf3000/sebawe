# -*- coding: utf-8 -*-
"""arabert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XIjZv_4we91kEV_zE1knfb87P3lRYQvH
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple, Set
from tqdm import tqdm
import re
import unicodedata
import numpy as np
import os

# --- 1. CONFIGURATION AND CONSTANTS ---
ARABERT_MODEL_NAME = 'aubmindlab/bert-base-arabertv2'
ARABIC_DIACRITICS = {
    '\u064e', '\u064f', '\u0650', '\u0652', '\u064b', '\u064c', '\u064d',
    '\u0651\u064e', '\u0651\u064f', '\u0651\u0650', '\u0651\u064b', '\u0651\u064c', '\u0651\u064d',
    '\u0651',
}
PAD_TOKEN = '<PAD>'
UNK_TOKEN = '<UNK>'
NO_DIACRITIC = '<NO_DIAC>'
SPACE_CHAR = ' '
TARGET_LABELS: Set[str] = ARABIC_DIACRITICS.union({NO_DIACRITIC, SPACE_CHAR})

MAX_TOKENS = 512
STRIDE = 0
IGNORE_INDEX = -100 # Standard PyTorch ignore index
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# --- 2. DATA CLEANING & UTILS (Kept Same) ---
def clean_sentence(sentence: str) -> str:
    sentence = unicodedata.normalize('NFKC', sentence)
    sentence = sentence.replace('\u0622', '\u0627').replace('\u0623', '\u0627').replace('\u0625', '\u0627').replace('\u0671', '\u0627')
    cleaned_text = re.sub(r'[^\u0621-\u064A\u064B-\u0652\s]', ' ', sentence)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

def separate_chars_and_diacritics(diacritized_sentence: str) -> Tuple[List[str], List[str]]:
    chars: List[str] = []
    diacritics: List[str] = []
    i = 0
    N = len(diacritized_sentence)
    # Regex for Arabic letters
    DIACRITIC_PATTERN = re.compile(f"[{''.join(ARABIC_DIACRITICS)}]")

    while i < N:
        char = diacritized_sentence[i]
        if char == SPACE_CHAR:
            chars.append(char); diacritics.append(SPACE_CHAR); i += 1
            continue
        if re.match(r'[\u0621-\u064a]', char):
            chars.append(char)
            current_diacritic = ''
            j = i + 1
            if j < N and diacritized_sentence[j] == '\u0651':
                current_diacritic += diacritized_sentence[j]; j += 1
            if j < N and diacritized_sentence[j] in ARABIC_DIACRITICS:
                current_diacritic += diacritized_sentence[j]; j += 1

            if current_diacritic in ARABIC_DIACRITICS: diacritics.append(current_diacritic)
            else: diacritics.append(NO_DIACRITIC)
            i = j if current_diacritic else i + 1
        else: i += 1
    return chars, diacritics

def create_vocabs(all_chars, all_diacs):
    char_set = set(all_chars)
    char_vocab_list = [PAD_TOKEN, UNK_TOKEN, SPACE_CHAR] + sorted(list(char_set - {PAD_TOKEN, UNK_TOKEN, SPACE_CHAR}))
    char_to_idx = {char: i for i, char in enumerate(char_vocab_list)}

    diac_vocab_list = [PAD_TOKEN] + sorted(list(TARGET_LABELS))
    diac_to_idx = {diac: i for i, diac in enumerate(diac_vocab_list)}

    return char_to_idx, diac_to_idx, len(char_vocab_list), len(diac_vocab_list)

def split_long_sequences(original_data, tokenizer, max_tokens=MAX_TOKENS, stride=STRIDE):
    max_tokens_segment = max_tokens - 2
    processed_data = []

    for chars, diacritics in tqdm(original_data, desc="Splitting sequences"):
        raw_text = "".join(chars)
        encoded_full = tokenizer(raw_text, return_offsets_mapping=True, truncation=False, add_special_tokens=True)
        N_tokens = len(encoded_full['input_ids'])
        full_offsets = encoded_full['offset_mapping'] # list of tuples of inidices indicating the start and end in the original sentence

        if N_tokens <= max_tokens:
            processed_data.append((chars, diacritics))
            continue

        start_token_idx = 1 # Skip CLS
        while start_token_idx < N_tokens - 1:
            end_token_idx = min(start_token_idx + max_tokens_segment, N_tokens - 1)

            char_start = full_offsets[start_token_idx][0]
            if end_token_idx == N_tokens - 1: char_end = len(chars)
            else: char_end = full_offsets[end_token_idx + 1][0] # Start of next token

            # Sanity check bounds
            char_start = max(0, min(char_start, len(chars)))
            char_end = max(0, min(char_end, len(chars)))

            if char_end > char_start:
                processed_data.append((chars[char_start:char_end], diacritics[char_start:char_end]))

            if end_token_idx == N_tokens - 1: break
            start_token_idx = end_token_idx - stride + 1

    return processed_data

# --- 3. DATASET (Alignment Logic : character embedding + arabert output of the corresponding token) ---
class HybridBiLSTMDataset(Dataset):
    def __init__(self, data: List[Tuple[List[str], List[str]]], tokenizer: AutoTokenizer, char_to_idx: Dict[str, int], diac_to_idx: Dict[str, int]):
        self.data = data
        self.tokenizer = tokenizer
        self.char_to_idx = char_to_idx
        self.diac_to_idx = diac_to_idx
        self.unk_char_idx = char_to_idx[UNK_TOKEN]
        self.pad_char_idx = char_to_idx[PAD_TOKEN]

    def __len__(self): return len(self.data)

    def __getitem__(self, idx: int):
        chars, diacritics = self.data[idx]
        raw_text = "".join(chars)

        # 1. Character Indices & Labels
        char_ids = [self.char_to_idx.get(c, self.unk_char_idx) for c in chars]
        # Use -100 (IGNORE_INDEX) for padding labels later, but here we just map
        target_ids = [self.diac_to_idx[d] for d in diacritics]

        # 2. AraBERT Tokenization
        encoded = self.tokenizer(
            raw_text,
            add_special_tokens=True,
            return_offsets_mapping=True,
            truncation=True,
            max_length=MAX_TOKENS
        )
        bert_input_ids = encoded['input_ids']
        bert_attention_mask = encoded['attention_mask']
        offsets = encoded['offset_mapping']

        # 3. Create Map: Char Index -> BERT Token Index
        # We need a list 'mapping' of length len(chars).
        # mapping[i] = k means character i is covered by BERT token k.

        char_to_bert_map = [0] * len(chars)

        # Iterate over offsets to fill the map
        for token_idx, (start, end) in enumerate(offsets):
            # Skip special tokens [CLS], [SEP] which have (0,0) usually or cover nothing relevant
            if start == end: continue

            # BERT offsets are based on the raw string.
            # Since chars is just list(raw_string), indices align perfectly.
            # We clip indices just in case tokenizer does something weird at boundaries
            safe_start = max(0, min(start, len(chars) - 1))
            safe_end = max(0, min(end, len(chars)))

            for c_i in range(safe_start, safe_end):
                char_to_bert_map[c_i] = token_idx

        return {
            'char_ids': torch.tensor(char_ids, dtype=torch.long),
            'targets': torch.tensor(target_ids, dtype=torch.long),
            'bert_input_ids': torch.tensor(bert_input_ids, dtype=torch.long),
            'bert_attention_mask': torch.tensor(bert_attention_mask, dtype=torch.long),
            'char_to_bert_map': torch.tensor(char_to_bert_map, dtype=torch.long),
            'length': len(chars) # For packing LSTM
        }

def collate_batch_bilstm(batch):
    # Sort by character length (descending) for pack_padded_sequence
    batch.sort(key=lambda x: x['length'], reverse=True)

    char_ids = [item['char_ids'] for item in batch]
    targets = [item['targets'] for item in batch]
    bert_ids = [item['bert_input_ids'] for item in batch]
    bert_mask = [item['bert_attention_mask'] for item in batch]
    maps = [item['char_to_bert_map'] for item in batch]
    lengths = torch.tensor([item['length'] for item in batch], dtype=torch.long)

    # Pad Character Inputs (Padding Value = 0 or PAD_IDX)
    char_ids_pad = pad_sequence(char_ids, batch_first=True, padding_value=0).to(DEVICE)

    # Pad Targets (Padding Value = -100 for CrossEntropyLoss ignore)
    targets_pad = pad_sequence(targets, batch_first=True, padding_value=IGNORE_INDEX).to(DEVICE)

    # Pad BERT Inputs (Standard BERT padding 0)
    bert_ids_pad = pad_sequence(bert_ids, batch_first=True, padding_value=0).to(DEVICE)
    bert_mask_pad = pad_sequence(bert_mask, batch_first=True, padding_value=0).to(DEVICE)

    # Pad Map (Points to index 0 [CLS] effectively, or use a dummy index if preferred)
    # If we pad with 0, padding chars will pull the [CLS] embedding, which is fine as they are masked out anyway.
    maps_pad = pad_sequence(maps, batch_first=True, padding_value=0).to(DEVICE)

    return {
        'char_ids': char_ids_pad,
        'targets': targets_pad,
        'bert_input_ids': bert_ids_pad,
        'bert_attention_mask': bert_mask_pad,
        'char_to_bert_map': maps_pad,
        'lengths': lengths
    }

# --- 4. THE MODEL: AraBERT + BiLSTM ---
class AraBertBiLSTM(nn.Module):
    def __init__(self, char_vocab_size, char_emb_dim, hidden_dim, lstm_layers, num_classes, dropout=0.3):
        super().__init__()

        # 1. AraBERT (Feature Extractor)
        self.bert = AutoModel.from_pretrained(ARABERT_MODEL_NAME)
        self.bert_hidden_size = self.bert.config.hidden_size

        # Optional: Freeze BERT to save memory/time, or unfreeze for better performance
        # for param in self.bert.parameters(): param.requires_grad = False

        # 2. Character Embedding
        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)

        # 3. BiLSTM
        # Input size = BERT Hidden (768) + Char Emb Dim
        self.lstm_input_dim = self.bert_hidden_size + char_emb_dim

        self.lstm = nn.LSTM(
            input_size=self.lstm_input_dim,
            hidden_size=hidden_dim,
            num_layers=lstm_layers,
            bidirectional=True,
            batch_first=True,
            dropout=dropout if lstm_layers > 1 else 0
        )

        # 4. Classifier
        self.classifier = nn.Linear(hidden_dim * 2, num_classes) # *2 for bidirectional
        self.dropout = nn.Dropout(dropout)

    def forward(self, char_ids, bert_input_ids, bert_attention_mask, char_to_bert_map, lengths):
        # A. Get BERT Outputs
        # shape: (Batch, Bert_Seq_Len, 768)
        bert_outputs = self.bert(input_ids=bert_input_ids, attention_mask=bert_attention_mask).last_hidden_state

        # B. Align BERT Outputs to Characters
        # We want shape: (Batch, Char_Seq_Len, 768)
        # char_to_bert_map is (Batch, Char_Seq_Len). We need to expand it to gather vectors.

        batch_size, char_seq_len = char_ids.size()

        # Create batch indices for gathering: [[0,0,..], [1,1,..], ..]
        batch_indices = torch.arange(batch_size, device=char_ids.device).unsqueeze(1).expand(-1, char_seq_len)

        # Gather: Select specific tokens from BERT output based on the map
        # aligned_bert_emb[b, t, :] = bert_outputs[b, map[b, t], :]
        aligned_bert_emb = bert_outputs[batch_indices, char_to_bert_map, :]

        # C. Get Character Embeddings
        # shape: (Batch, Char_Seq_Len, Char_Emb_Dim)
        char_embs = self.char_embedding(char_ids)

        # D. Concatenate Features
        # shape: (Batch, Char_Seq_Len, 768 + Char_Emb_Dim)
        combined_features = torch.cat([aligned_bert_emb, char_embs], dim=2)
        combined_features = self.dropout(combined_features)

        # E. Run BiLSTM
        # Pack sequence for efficiency and correctness (ignores padding steps)
        packed_input = pack_padded_sequence(combined_features, lengths.cpu(), batch_first=True, enforce_sorted=True)
        packed_output, _ = self.lstm(packed_input)
        lstm_output, _ = pad_packed_sequence(packed_output, batch_first=True)

        # F. Classification
        logits = self.classifier(self.dropout(lstm_output))

        return logits

# --- 5. TRAINING LOOP (Updated) ---
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, name):
    best_acc = 0.0
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)

    for epoch in range(num_epochs):
        model.train()
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]", leave=False)
        train_loss = 0
        correct = 0
        total_tokens = 0

        for batch in loop:
            # Move all to device done in collate, just unpack
            char_ids = batch['char_ids']
            targets = batch['targets']
            bert_ids = batch['bert_input_ids']
            bert_mask = batch['bert_attention_mask']
            maps = batch['char_to_bert_map']
            lens = batch['lengths']

            optimizer.zero_grad()

            logits = model(char_ids, bert_ids, bert_mask, maps, lens)

            # Flatten for Loss (Batch * Seq, Classes)
            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))

            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            train_loss += loss.item()

            # Accuracy Calc
            preds = torch.argmax(logits, dim=-1)
            mask = (targets != IGNORE_INDEX)
            correct += (preds == targets).masked_select(mask).sum().item()
            total_tokens += mask.sum().item()

            loop.set_postfix(loss=loss.item())

        train_acc = (correct / total_tokens) * 100 if total_tokens > 0 else 0

        # Validation
        model.eval()
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for batch in val_loader:
                char_ids = batch['char_ids']
                targets = batch['targets']
                bert_ids = batch['bert_input_ids']
                bert_mask = batch['bert_attention_mask']
                maps = batch['char_to_bert_map']
                lens = batch['lengths']

                logits = model(char_ids, bert_ids, bert_mask, maps, lens)

                preds = torch.argmax(logits, dim=-1)
                mask = (targets != IGNORE_INDEX)
                val_correct += (preds == targets).masked_select(mask).sum().item()
                val_total += mask.sum().item()

        val_acc = (val_correct / val_total) * 100 if val_total > 0 else 0
        scheduler.step(val_acc)

        print(f"[{name}] Epoch {epoch+1}: Loss: {train_loss/len(train_loader):.4f} | Tr Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), f"{name}_best_bilstm.pkl")


def load_data(path):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            sent = f.read().splitlines()
    except:
        sent = ["ذهب الولد الى المدرسة", "بسم الله"] * 20
    proc, ac, ad = [], [], []
    for s in sent:
        cs = clean_sentence(s)
        if not cs:
            continue
        c, d = separate_chars_and_diacritics(cs)
        if c:
            proc.append((c, d))
            ac.extend(c)
            ad.extend(d)
    return proc, ac, ad

STRIDE = 0

# 1. Load & Process
print("Loading Data...")
# Update paths as needed
tr_raw, tc, td = load_data('../data/train.txt')
va_raw, vc, vd = load_data('../data/val.txt')

c2i, d2i, CV, DV = create_vocabs(tc+vc, td+vd)
print(f"Vocabs: Char={CV}, Diac={DV}")

tokenizer = AutoTokenizer.from_pretrained(ARABERT_MODEL_NAME)

# 2. Split Sequences
tr_split = split_long_sequences(tr_raw, tokenizer)
va_split = split_long_sequences(va_raw, tokenizer)

# 3. Create Datasets & Loaders
train_ds = HybridBiLSTMDataset(tr_split, tokenizer, c2i, d2i)
val_ds = HybridBiLSTMDataset(va_split, tokenizer, c2i, d2i)

# the first output above is just a warning . I printed the lengthes before and after splitting to verify it worked
print(f"Original sentences: {len(tr_raw)}")
print(f"Segments after splitting: {len(tr_split)}")

# If tr_data_split is larger than tr_data_full, splitting worked!

# Hyperparameters
HP = {
    'C_EMB': 64,        # Char Embedding Dim
    'HIDDEN': 256,      # LSTM Hidden Dim
    'LAYERS': 2,        # LSTM Layers
    'LR': 2e-5,         # Learning Rate (Low because BERT is trainable)
    'EPOCHS': 5,
    'BS': 32,
}

train_loader = DataLoader(train_ds, batch_size=HP['BS'], shuffle=True, collate_fn=collate_batch_bilstm)
val_loader = DataLoader(val_ds, batch_size=HP['BS'], shuffle=False, collate_fn=collate_batch_bilstm)

# 4. Init Model
model = AraBertBiLSTM(
    char_vocab_size=CV,
    char_emb_dim=HP['C_EMB'],
    hidden_dim=HP['HIDDEN'],
    lstm_layers=HP['LAYERS'],
    num_classes=DV
).to(DEVICE)

# 5. Train
criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)
optimizer = optim.AdamW(model.parameters(), lr=HP['LR'])

print("Starting Training with Feature Vector: [Char_Emb + AraBert_Token_Emb] -> BiLSTM")
train_model(model, train_loader, val_loader, criterion, optimizer, HP['EPOCHS'], "BiLSTM_BertFeat")

# Hyperparameters
HP = {
    'C_EMB': 64,        # Char Embedding Dim
    'HIDDEN': 256,      # LSTM Hidden Dim
    'LAYERS': 2,        # LSTM Layers
    'LR': 2e-5,         # Learning Rate (Low because BERT is trainable)
    'EPOCHS': 5,
    'BS': 32,
}

# 2. Split Sequences
tr_split = split_long_sequences(tr_raw, tokenizer, stride=128)
va_split = split_long_sequences(va_raw, tokenizer, stride=0)

# 3. Create Datasets & Loaders
train_ds = HybridBiLSTMDataset(tr_split, tokenizer, c2i, d2i)
val_ds = HybridBiLSTMDataset(va_split, tokenizer, c2i, d2i)

train_loader = DataLoader(train_ds, batch_size=HP['BS'], shuffle=True, collate_fn=collate_batch_bilstm)
val_loader = DataLoader(val_ds, batch_size=HP['BS'], shuffle=False, collate_fn=collate_batch_bilstm)

# 4. Init Model
model = AraBertBiLSTM(
    char_vocab_size=CV,
    char_emb_dim=HP['C_EMB'],
    hidden_dim=HP['HIDDEN'],
    lstm_layers=HP['LAYERS'],
    num_classes=DV
).to(DEVICE)

# 5. Train
criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)
optimizer = optim.AdamW(model.parameters(), lr=HP['LR'])

print("Starting Training with Feature Vector: [Char_Emb + AraBert_Token_Emb] -> BiLSTM")
train_model(model, train_loader, val_loader, criterion, optimizer, HP['EPOCHS'], "BiLSTM_BertFeat")
