# -*- coding: utf-8 -*-
"""notebook3a24e07b2d.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kuVPVjBlLFSYypeH97yymejdMYMRXlXP
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import KFold
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import re
import pickle
import unicodedata
import numpy as np
from typing import List, Dict, Tuple, Set
from tqdm import tqdm

# --- NEW IMPORTS FOR W2V & KMEANS ---
from gensim.models import Word2Vec
from sklearn.cluster import KMeans

# --- 1. CONFIGURATION AND CONSTANTS ---
ARABIC_DIACRITICS = {
    '\u064e', '\u064f', '\u0650', '\u0652', '\u064b', '\u064c', '\u064d',
    '\u0651\u064e', '\u0651\u064f', '\u0651\u0650', '\u0651\u064b', '\u0651\u064c', '\u0651\u064d',
    '\u0651',
}
PAD_TOKEN = '<PAD>'
UNK_TOKEN = '<UNK>'
NO_DIACRITIC = '<NO_DIAC>'
SPACE_CHAR = ' '
TARGET_LABELS: Set[str] = ARABIC_DIACRITICS.union({NO_DIACRITIC, SPACE_CHAR})

IGNORE_INDEX = 0
DIACRITIC_PATTERN = re.compile(f"[{''.join(ARABIC_DIACRITICS)}]")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# --- 2. DATA CLEANING & UTILS ---
def clean_sentence(sentence: str) -> str:
    sentence = unicodedata.normalize('NFKC', sentence)
    sentence = sentence.replace('\u0622', '\u0627').replace('\u0623', '\u0627').replace('\u0625', '\u0627').replace('\u0671', '\u0627')
    cleaned_text = re.sub(r'[^\u0621-\u064A\u064B-\u0652\s]', ' ', sentence)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

def separate_chars_and_diacritics(diacritized_sentence: str):
    chars: List[str] = []
    diacritics: List[str] = []
    i = 0
    N = len(diacritized_sentence)
    while i < N:
        char = diacritized_sentence[i]
        if char == SPACE_CHAR:
            chars.append(char); diacritics.append(SPACE_CHAR); i += 1
            continue
        if re.match(r'[\u0621-\u064a]', char):
            chars.append(char)
            current_diacritic = ''
            j = i + 1
            if j < N and diacritized_sentence[j] == '\u0651':
                current_diacritic += diacritized_sentence[j]; j += 1
            if j < N and diacritized_sentence[j] in DIACRITIC_PATTERN.pattern:
                current_diacritic += diacritized_sentence[j]; j += 1
            if current_diacritic in ARABIC_DIACRITICS: diacritics.append(current_diacritic)
            else: diacritics.append(NO_DIACRITIC)
            i = j if current_diacritic else i + 1
        else: i += 1
    return chars, diacritics

def create_vocabs(all_chars, all_diacs, all_words):
    char_set = set(all_chars)
    char_vocab_list = [PAD_TOKEN, UNK_TOKEN, SPACE_CHAR] + sorted(list(char_set - {PAD_TOKEN, UNK_TOKEN, SPACE_CHAR}))
    char_to_idx = {char: i for i, char in enumerate(char_vocab_list)}

    diac_vocab_list = [PAD_TOKEN] + sorted(list(TARGET_LABELS))
    diac_to_idx = {diac: i for i, diac in enumerate(diac_vocab_list)}

    word_set = set(all_words)
    word_vocab_list = [PAD_TOKEN, UNK_TOKEN] + sorted(list(word_set - {PAD_TOKEN, UNK_TOKEN}))
    word_to_idx = {word: i for i, word in enumerate(word_vocab_list)}

    return char_to_idx, diac_to_idx, word_to_idx, len(char_vocab_list), len(diac_vocab_list), len(word_vocab_list)

# --- 3. NEW: W2V + K-MEANS PIPELINE ---
def train_w2v_kmeans_clusters(sentences_words: List[List[str]], num_clusters=50, vector_size=100):
    print("  >>> Training Word2Vec...")
    # Train Word2Vec on the list of tokenized sentences
    w2v_model = Word2Vec(sentences=sentences_words, vector_size=vector_size, window=5, min_count=1, workers=4)

    print("  >>> Training K-Means...")
    # Extract vectors for all words in vocab
    words = list(w2v_model.wv.index_to_key)
    vectors = [w2v_model.wv[word] for word in words]

    # Train KMeans
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
    kmeans.fit(vectors)
    labels = kmeans.labels_

    # Create Mapping: Word -> Cluster ID
    # Cluster IDs start at 0, we shift them to 1 so 0 can be used for padding/unknown
    word_to_cluster = {word: int(label) + 1 for word, label in zip(words, labels)}

    # Add special tokens to cluster 0 (Unknown/Pad)
    word_to_cluster[PAD_TOKEN] = 0
    word_to_cluster[UNK_TOKEN] = 0
    word_to_cluster[SPACE_CHAR] = 0

    print(f"  >>> Clusters Assigned. Vocab size: {len(word_to_cluster)}")
    return word_to_cluster

# --- 4. DATASET UPDATED FOR CLUSTERS ---
class ArabicDiacritizationDataset(Dataset):
    def __init__(self, data, char_to_idx, diac_to_idx, word_to_idx, word_to_cluster):
        self.data = data
        self.char_to_idx = char_to_idx
        self.diac_to_idx = diac_to_idx
        self.word_to_idx = word_to_idx
        self.word_to_cluster = word_to_cluster # NEW MAPPING

        self.unk_char_idx = char_to_idx[UNK_TOKEN]
        self.unk_word_idx = word_to_idx[UNK_TOKEN]
        self.space_word_idx = word_to_idx.get(SPACE_CHAR, self.unk_word_idx)

    def __len__(self): return len(self.data)

    def __getitem__(self, idx: int):
        chars, diacritics = self.data[idx]
        seq_len = len(chars)

        x_char_indices = [self.char_to_idx.get(c, self.unk_char_idx) for c in chars]
        y_indices = [self.diac_to_idx.get(d, self.diac_to_idx[NO_DIACRITIC]) for d in diacritics]

        x_word_indices = []
        x_cluster_indices = [] # NEW FEATURE VECTOR
        x_last_char_flag = []

        current_word_chars = []

        for i, char in enumerate(chars):
            if char == SPACE_CHAR:
                if current_word_chars:
                    full_word = "".join(current_word_chars)

                    # 1. Word ID
                    word_idx = self.word_to_idx.get(full_word, self.unk_word_idx)
                    x_word_indices.extend([word_idx] * len(current_word_chars))

                    # 2. Cluster ID (New Feature)
                    # Default to 0 if word not in clustering map
                    cluster_idx = self.word_to_cluster.get(full_word, 0)
                    x_cluster_indices.extend([cluster_idx] * len(current_word_chars))

                    # 3. Last Char Flag
                    flags = [0] * len(current_word_chars); flags[-1] = 1
                    x_last_char_flag.extend(flags)

                    current_word_chars = []

                # Handle Space
                x_word_indices.append(self.space_word_idx)
                x_cluster_indices.append(0) # Cluster 0 for space
                x_last_char_flag.append(0)
            else:
                current_word_chars.append(char)

        # Handle last word if no trailing space
        if current_word_chars:
            full_word = "".join(current_word_chars)
            word_idx = self.word_to_idx.get(full_word, self.unk_word_idx)
            x_word_indices.extend([word_idx] * len(current_word_chars))

            cluster_idx = self.word_to_cluster.get(full_word, 0)
            x_cluster_indices.extend([cluster_idx] * len(current_word_chars))

            flags = [0] * len(current_word_chars); flags[-1] = 1
            x_last_char_flag.extend(flags)

        return (
            torch.tensor(x_char_indices),
            torch.tensor(x_word_indices),
            torch.tensor(x_cluster_indices), # Return cluster tensor
            torch.tensor(x_last_char_flag, dtype=torch.float),
            torch.tensor(y_indices),
            seq_len
        )

def collate_batch(batch):
    batch.sort(key=lambda x: x[5], reverse=True) # Sort by seq_len
    X_char, X_word, X_cluster, X_last, Y, lengths = zip(*batch)

    # Pad everything
    X_char_pad = torch.nn.utils.rnn.pad_sequence(X_char, batch_first=True, padding_value=IGNORE_INDEX).to(DEVICE)
    X_word_pad = torch.nn.utils.rnn.pad_sequence(X_word, batch_first=True, padding_value=IGNORE_INDEX).to(DEVICE)
    X_clust_pad = torch.nn.utils.rnn.pad_sequence(X_cluster, batch_first=True, padding_value=0).to(DEVICE) # NEW
    Y_pad = torch.nn.utils.rnn.pad_sequence(Y, batch_first=True, padding_value=IGNORE_INDEX).to(DEVICE)
    X_last_pad = torch.nn.utils.rnn.pad_sequence(X_last, batch_first=True, padding_value=0.0).to(DEVICE)
    len_tens = torch.tensor(lengths, dtype=torch.long).to(DEVICE)

    return X_char_pad, X_word_pad, X_clust_pad, X_last_pad, Y_pad, len_tens

# --- 5. UPDATED FEATURE EXTRACTOR ---
class FeatureExtractor(nn.Module):
    def __init__(self, char_vocab_size, word_vocab_size, num_clusters,
                 char_emb_dim, word_emb_dim, cluster_emb_dim,
                 use_word_emb, use_cluster_emb, use_last_char_flag):
        super().__init__()

        self.use_word_emb = use_word_emb
        self.use_cluster_emb = use_cluster_emb # NEW FLAG
        self.use_last_char_flag = use_last_char_flag

        # Char Embedding (Always used)
        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=IGNORE_INDEX)
        output_dim = char_emb_dim

        # Optional: Word Embedding
        if use_word_emb:
            self.word_embedding = nn.Embedding(word_vocab_size, word_emb_dim, padding_idx=IGNORE_INDEX)
            output_dim += word_emb_dim

        # Optional: Cluster Embedding (NEW)
        if use_cluster_emb:
            # num_clusters + 1 because 0 is padding/unknown
            self.cluster_embedding = nn.Embedding(num_clusters + 1, cluster_emb_dim, padding_idx=0)
            output_dim += cluster_emb_dim

        # Optional: Handcrafted Feature
        if use_last_char_flag:
            output_dim += 1

        self.output_dim = output_dim

    def forward(self, x_char, x_word, x_cluster, x_last):
        embedded_features = [self.char_embedding(x_char)]

        if self.use_word_emb:
            embedded_features.append(self.word_embedding(x_word))

        if self.use_cluster_emb:
            embedded_features.append(self.cluster_embedding(x_cluster))

        if self.use_last_char_flag:
            embedded_features.append(x_last.unsqueeze(-1))

        return torch.cat(embedded_features, dim=-1)

# --- 6. MODEL (BiLSTM) ---
class DiacritizerBiLSTM(nn.Module):
    def __init__(self, feature_extractor, hidden_dim, num_layers, diac_vocab_size, dropout_rate=0.5):
        super().__init__()
        self.feature_extractor = feature_extractor
        self.lstm = nn.LSTM(
            input_size=feature_extractor.output_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_dim * 2, diac_vocab_size)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x_char, x_word, x_cluster, x_last, lengths):
        # Pass all inputs to extractor
        embedded = self.dropout(self.feature_extractor(x_char, x_word, x_cluster, x_last))

        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=True)
        packed_output, _ = self.lstm(packed_embedded)
        output, _ = pad_packed_sequence(packed_output, batch_first=True)
        return self.fc(output)

# --- 7. TRAIN LOOP ---
def calculate_accuracy(logits, targets):
    predictions = torch.argmax(logits, dim=-1)
    mask = (targets != IGNORE_INDEX)
    total = torch.sum(mask).float()
    if total == 0: return 0.0
    return (torch.sum((predictions == targets) & mask).float() / total).item() * 100

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, name):
    best_acc = 0.0
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)

    for epoch in range(num_epochs):
        model.train()
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]", leave=False)
        train_loss, train_acc = 0, 0

        for batch in loop:
            # Unpack 6 items now
            Xc, Xw, Xcl, Xl, Y, Lens = batch
            optimizer.zero_grad()
            logits = model(Xc, Xw, Xcl, Xl, Lens)
            loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item(); train_acc += calculate_accuracy(logits, Y)
            loop.set_postfix(loss=loss.item())

        # Validation
        model.eval()
        val_acc = 0
        with torch.no_grad():
            for batch in val_loader:
                Xc, Xw, Xcl, Xl, Y, Lens = batch
                logits = model(Xc, Xw, Xcl, Xl, Lens)
                val_acc += calculate_accuracy(logits, Y)

        avg_train_acc = train_acc / len(train_loader)
        avg_val_acc = val_acc / len(val_loader)
        scheduler.step(avg_val_acc)

        print(f"[{name}] Epoch {epoch+1}: Train: {avg_train_acc:.2f}% | Val: {avg_val_acc:.2f}%")
        if avg_val_acc > best_acc:
            best_acc = avg_val_acc
            torch.save(model.state_dict(), f"{name}_best.pkl")

    return best_acc

# --- 8. PIPELINE ---
def run_pipeline(name, use_word, use_cluster, use_flag, data_config, hp):
    print(f"\n=== Pipeline: {name} (Word={use_word}, Cluster={use_cluster}, Flag={use_flag}) ===")

    # Pass word_to_cluster mapping to dataset
    train_ds = ArabicDiacritizationDataset(data_config['train_data'], data_config['c2i'], data_config['d2i'], data_config['w2i'], data_config['w2c'])
    val_ds = ArabicDiacritizationDataset(data_config['val_data'], data_config['c2i'], data_config['d2i'], data_config['w2i'], data_config['w2c'])

    train_loader = DataLoader(train_ds, batch_size=hp['BS'], shuffle=True, collate_fn=collate_batch)
    val_loader = DataLoader(val_ds, batch_size=hp['BS'], shuffle=False, collate_fn=collate_batch)

    feature_extractor = FeatureExtractor(
        data_config['CV_SIZE'], data_config['WV_SIZE'], hp['NUM_CLUSTERS'],
        hp['C_EMB'], hp['W_EMB'], hp['CL_EMB'],
        use_word, use_cluster, use_flag
    )

    model = DiacritizerBiLSTM(feature_extractor, hp['HIDDEN'], hp['LAYERS'], data_config['DV_SIZE'], hp['DROP']).to(DEVICE)
    criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)
    optimizer = optim.Adam(model.parameters(), lr=hp['LR'])

    return train_model(model, train_loader, val_loader, criterion, optimizer, hp['EPOCHS'], name)

# --- 9. MAIN ---
def load_data(path):
    try:
        with open(path, 'r', encoding='utf-8') as f: sent = f.read().splitlines()
    except: sent = ["ذهب الولد", "بسم الله"] * 20
    proc, ch, di, wo, raw_wo_sent = [], [], [], [], []
    for s in sent:
        cs = clean_sentence(s)
        if not cs: continue
        c, d = separate_chars_and_diacritics(cs)
        if c:
            proc.append((c, d))
            ch.extend(c); di.extend(d);
            words = "".join(c).split(SPACE_CHAR)
            wo.extend(words)
            raw_wo_sent.append(words) # Store strictly tokenized sentences for W2V
    return proc, ch, di, wo, raw_wo_sent


HP = {
    'C_EMB': 128, 'W_EMB': 256, 'CL_EMB': 32, # Cluster embedding size
    'HIDDEN': 256, 'LAYERS': 3, 'DROP': 0.5,
    'LR': 0.001, 'EPOCHS': 25, 'BS': 64,
    'NUM_CLUSTERS': 50 # Number of KMeans clusters
}

# Load Data
tr_data, t_c, t_d, t_w, t_raw_sentences = load_data('../data/train.txt')
va_data, v_c, v_d, v_w, _ = load_data('../data/val.txt')

# Build Vocabs
c2i, d2i, w2i, CV, DV, WV = create_vocabs(t_c+v_c, t_d+v_d, t_w+v_w)

# TRAIN WORD2VEC & KMEANS (On training data only!)
print("Generating Semantic Clusters...")
word_to_cluster_map = train_w2v_kmeans_clusters(
    t_raw_sentences,
    num_clusters=HP['NUM_CLUSTERS'],
    vector_size=100
)

CONFIG = {
    'train_data': tr_data, 'val_data': va_data,
    'c2i': c2i, 'd2i': d2i, 'w2i': w2i, 'w2c': word_to_cluster_map,
    'CV_SIZE': CV, 'DV_SIZE': DV, 'WV_SIZE': WV
}

# --- Ablation Study: All combinations of (Word, Cluster, Flag) ---
# Legend: W=Word Emb, K=KMeans Cluster Emb, F=Last Char Flag

# # 1. Chars only (Baseline)
# run_pipeline("Chars_Only", False, False, False, CONFIG, HP)

# # 2. Chars + Flag
# run_pipeline("Chars_Flag", False, False, True, CONFIG, HP)

# 8. Chars + Words + Clusters + Flag (Full Model)
# run_pipeline("Full_Features", True, True, True, CONFIG, HP)

# # 7. Chars + Words + Clusters (Words + Semantic Feature)
# run_pipeline("Words_KMeans", True, True, False, CONFIG, HP)

# # 6. Chars + Words + Flag
# run_pipeline("Words_Flag", True, False, True, CONFIG, HP)

# # 3. Chars + Clusters
# run_pipeline("Chars_KMeans", False, True, False, CONFIG, HP)

# 4. Chars + Clusters + Flag
run_pipeline("Chars_KMeans_Flag", False, True, True, CONFIG, HP)

# 5. Chars + Words (Standard NLP Baseline)
run_pipeline("Chars_Words", True, False, False, CONFIG, HP)
