# -*- coding: utf-8 -*-
"""notebookfcd10561ae.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZDkYAThh7ik6VV8OtTfsvtPPmrlbVWT0
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
import re
import pickle
import unicodedata
import numpy as np
from typing import List, Dict, Tuple, Set
from tqdm import tqdm

# --- NEW IMPORTS FOR W2V & KMEANS ---
from gensim.models import Word2Vec
from sklearn.cluster import KMeans

# --- 1. CONFIGURATION AND CONSTANTS ---
ARABIC_DIACRITICS = {
    '\u064e', '\u064f', '\u0650', '\u0652', '\u064b', '\u064c', '\u064d',
    '\u0651\u064e', '\u0651\u064f', '\u0651\u0650', '\u0651\u064b', '\u0651\u064c', '\u0651\u064d',
    '\u0651',
}
PAD_TOKEN = '<PAD>'
UNK_TOKEN = '<UNK>'
NO_DIACRITIC = '<NO_DIAC>'
SPACE_CHAR = ' '
TARGET_LABELS: Set[str] = ARABIC_DIACRITICS.union({NO_DIACRITIC, SPACE_CHAR})

IGNORE_INDEX = 0
DIACRITIC_PATTERN = re.compile(f"[{''.join(ARABIC_DIACRITICS)}]")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

# --- 2. DATA CLEANING & UTILS ---
def clean_sentence(sentence: str) -> str:
    sentence = unicodedata.normalize('NFKC', sentence)
    sentence = sentence.replace('\u0622', '\u0627').replace('\u0623', '\u0627').replace('\u0625', '\u0627').replace('\u0671', '\u0627')
    cleaned_text = re.sub(r'[^\u0621-\u064A\u064B-\u0652\s]', ' ', sentence)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

def separate_chars_and_diacritics(diacritized_sentence: str):
    chars: List[str] = []
    diacritics: List[str] = []
    i = 0
    N = len(diacritized_sentence)
    while i < N:
        char = diacritized_sentence[i]
        if char == SPACE_CHAR:
            chars.append(char); diacritics.append(SPACE_CHAR); i += 1
            continue
        if re.match(r'[\u0621-\u064a]', char):
            chars.append(char)
            current_diacritic = ''
            j = i + 1
            if j < N and diacritized_sentence[j] == '\u0651':
                current_diacritic += diacritized_sentence[j]; j += 1
            if j < N and diacritized_sentence[j] in DIACRITIC_PATTERN.pattern:
                current_diacritic += diacritized_sentence[j]; j += 1
            if current_diacritic in ARABIC_DIACRITICS: diacritics.append(current_diacritic)
            else: diacritics.append(NO_DIACRITIC)
            i = j if current_diacritic else i + 1
        else: i += 1
    return chars, diacritics

def create_vocabs(all_chars, all_diacs, all_words):
    char_set = set(all_chars)
    char_vocab_list = [PAD_TOKEN, UNK_TOKEN, SPACE_CHAR] + sorted(list(char_set - {PAD_TOKEN, UNK_TOKEN, SPACE_CHAR}))
    char_to_idx = {char: i for i, char in enumerate(char_vocab_list)}

    diac_vocab_list = [PAD_TOKEN] + sorted(list(TARGET_LABELS))
    diac_to_idx = {diac: i for i, diac in enumerate(diac_vocab_list)}

    word_set = set(all_words)
    word_vocab_list = [PAD_TOKEN, UNK_TOKEN] + sorted(list(word_set - {PAD_TOKEN, UNK_TOKEN}))
    word_to_idx = {word: i for i, word in enumerate(word_vocab_list)}

    return char_to_idx, diac_to_idx, word_to_idx, len(char_vocab_list), len(diac_vocab_list), len(word_vocab_list)

# --- 3. W2V + K-MEANS PIPELINE ---
def train_w2v_kmeans_clusters(sentences_words: List[List[str]], num_clusters=50, vector_size=100):
    print("  >>> Training Word2Vec...")
    w2v_model = Word2Vec(sentences=sentences_words, vector_size=vector_size, window=5, min_count=1, workers=4)

    print("  >>> Training K-Means...")
    words = list(w2v_model.wv.index_to_key)
    vectors = [w2v_model.wv[word] for word in words]

    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
    kmeans.fit(vectors)
    labels = kmeans.labels_

    # Map Word -> Cluster ID (Shift by +1 to reserve 0 for padding)
    word_to_cluster = {word: int(label) + 1 for word, label in zip(words, labels)}

    word_to_cluster[PAD_TOKEN] = 0
    word_to_cluster[UNK_TOKEN] = 0
    word_to_cluster[SPACE_CHAR] = 0

    print(f"  >>> Clusters Assigned. Vocab size: {len(word_to_cluster)}")
    return word_to_cluster

# --- 4. DATASET ---
class ArabicDiacritizationDataset(Dataset):
    def __init__(self, data, char_to_idx, diac_to_idx, word_to_idx, word_to_cluster):
        self.data = data
        self.char_to_idx = char_to_idx
        self.diac_to_idx = diac_to_idx
        self.word_to_idx = word_to_idx
        self.word_to_cluster = word_to_cluster

        self.unk_char_idx = char_to_idx[UNK_TOKEN]
        self.unk_word_idx = word_to_idx[UNK_TOKEN]
        self.space_word_idx = word_to_idx.get(SPACE_CHAR, self.unk_word_idx)

    def __len__(self): return len(self.data)

    def __getitem__(self, idx: int):
        chars, diacritics = self.data[idx]
        seq_len = len(chars)

        x_char_indices = [self.char_to_idx.get(c, self.unk_char_idx) for c in chars]
        y_indices = [self.diac_to_idx.get(d, self.diac_to_idx[NO_DIACRITIC]) for d in diacritics]

        x_word_indices = []
        x_cluster_indices = []
        x_last_char_flag = []

        current_word_chars = []

        for i, char in enumerate(chars):
            if char == SPACE_CHAR:
                if current_word_chars:
                    full_word = "".join(current_word_chars)

                    word_idx = self.word_to_idx.get(full_word, self.unk_word_idx)
                    x_word_indices.extend([word_idx] * len(current_word_chars))

                    cluster_idx = self.word_to_cluster.get(full_word, 0)
                    x_cluster_indices.extend([cluster_idx] * len(current_word_chars))

                    flags = [0] * len(current_word_chars); flags[-1] = 1
                    x_last_char_flag.extend(flags)

                    current_word_chars = []

                x_word_indices.append(self.space_word_idx)
                x_cluster_indices.append(0)
                x_last_char_flag.append(0)
            else:
                current_word_chars.append(char)

        if current_word_chars:
            full_word = "".join(current_word_chars)
            word_idx = self.word_to_idx.get(full_word, self.unk_word_idx)
            x_word_indices.extend([word_idx] * len(current_word_chars))

            cluster_idx = self.word_to_cluster.get(full_word, 0)
            x_cluster_indices.extend([cluster_idx] * len(current_word_chars))

            flags = [0] * len(current_word_chars); flags[-1] = 1
            x_last_char_flag.extend(flags)

        return (
            torch.tensor(x_char_indices),
            torch.tensor(x_word_indices),
            torch.tensor(x_cluster_indices),
            torch.tensor(x_last_char_flag, dtype=torch.float),
            torch.tensor(y_indices),
            seq_len
        )

def collate_batch(batch):
    batch.sort(key=lambda x: x[5], reverse=True)
    X_char, X_word, X_cluster, X_last, Y, lengths = zip(*batch)

    X_char_pad = torch.nn.utils.rnn.pad_sequence(X_char, batch_first=True, padding_value=IGNORE_INDEX).to(DEVICE)
    X_word_pad = torch.nn.utils.rnn.pad_sequence(X_word, batch_first=True, padding_value=IGNORE_INDEX).to(DEVICE)
    X_clust_pad = torch.nn.utils.rnn.pad_sequence(X_cluster, batch_first=True, padding_value=0).to(DEVICE)
    Y_pad = torch.nn.utils.rnn.pad_sequence(Y, batch_first=True, padding_value=IGNORE_INDEX).to(DEVICE)
    X_last_pad = torch.nn.utils.rnn.pad_sequence(X_last, batch_first=True, padding_value=0.0).to(DEVICE)
    len_tens = torch.tensor(lengths, dtype=torch.long).to(DEVICE)

    return X_char_pad, X_word_pad, X_clust_pad, X_last_pad, Y_pad, len_tens

# --- 5. FEATURE EXTRACTOR ---
class FeatureExtractor(nn.Module):
    def __init__(self, char_vocab_size, word_vocab_size, num_clusters,
                 char_emb_dim, word_emb_dim, cluster_emb_dim,
                 use_word_emb, use_cluster_emb, use_last_char_flag):
        super().__init__()

        self.use_word_emb = use_word_emb
        self.use_cluster_emb = use_cluster_emb
        self.use_last_char_flag = use_last_char_flag

        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=IGNORE_INDEX)
        output_dim = char_emb_dim

        if use_word_emb:
            self.word_embedding = nn.Embedding(word_vocab_size, word_emb_dim, padding_idx=IGNORE_INDEX)
            output_dim += word_emb_dim

        if use_cluster_emb:
            self.cluster_embedding = nn.Embedding(num_clusters + 1, cluster_emb_dim, padding_idx=0)
            output_dim += cluster_emb_dim

        if use_last_char_flag:
            output_dim += 1

        self.output_dim = output_dim

    def forward(self, x_char, x_word, x_cluster, x_last):
        embedded_features = [self.char_embedding(x_char)]

        if self.use_word_emb:
            embedded_features.append(self.word_embedding(x_word))

        if self.use_cluster_emb:
            embedded_features.append(self.cluster_embedding(x_cluster))

        if self.use_last_char_flag:
            embedded_features.append(x_last.unsqueeze(-1))

        return torch.cat(embedded_features, dim=-1)

# --- 6. MODEL (BiRNN - Vanilla) ---
class DiacritizerBiRNN(nn.Module):
    def __init__(self, feature_extractor, hidden_dim, num_layers, diac_vocab_size, dropout_rate=0.5):
        super().__init__()
        self.feature_extractor = feature_extractor

        # --- CHANGED: LSTM -> RNN ---
        # nonlinearity='tanh' is standard for RNNs
        self.rnn = nn.RNN(
            input_size=feature_extractor.output_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            bidirectional=True,
            batch_first=True,
            nonlinearity='tanh',
            dropout=dropout_rate if num_layers > 1 else 0
        )

        self.fc = nn.Linear(hidden_dim * 2, diac_vocab_size)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x_char, x_word, x_cluster, x_last, lengths):
        # Pass all inputs to extractor
        embedded = self.dropout(self.feature_extractor(x_char, x_word, x_cluster, x_last))

        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=True)
        packed_output, _ = self.rnn(packed_embedded)
        output, _ = pad_packed_sequence(packed_output, batch_first=True)
        return self.fc(output)

# --- 7. TRAIN LOOP ---
def calculate_accuracy(logits, targets):
    predictions = torch.argmax(logits, dim=-1)
    mask = (targets != IGNORE_INDEX)
    total = torch.sum(mask).float()
    if total == 0: return 0.0
    return (torch.sum((predictions == targets) & mask).float() / total).item() * 100

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, name):
    best_acc = 0.0
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)

    for epoch in range(num_epochs):
        model.train()
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]", leave=False)
        train_loss, train_acc = 0, 0

        for batch in loop:
            Xc, Xw, Xcl, Xl, Y, Lens = batch
            optimizer.zero_grad()
            logits = model(Xc, Xw, Xcl, Xl, Lens)
            loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            train_loss += loss.item(); train_acc += calculate_accuracy(logits, Y)
            loop.set_postfix(loss=loss.item())

        model.eval()
        val_acc = 0
        with torch.no_grad():
            for batch in val_loader:
                Xc, Xw, Xcl, Xl, Y, Lens = batch
                logits = model(Xc, Xw, Xcl, Xl, Lens)
                val_acc += calculate_accuracy(logits, Y)

        avg_train_acc = train_acc / len(train_loader)
        avg_val_acc = val_acc / len(val_loader)
        scheduler.step(avg_val_acc)

        print(f"[{name}] Epoch {epoch+1}: Train: {avg_train_acc:.2f}% | Val: {avg_val_acc:.2f}%")
        if avg_val_acc > best_acc:
            best_acc = avg_val_acc
            torch.save(model.state_dict(), f"{name}_best.pkl")

    return best_acc

# --- 8. PIPELINE ---
def run_pipeline(name, use_word, use_cluster, use_flag, data_config, hp):
    print(f"\n=== BiRNN Pipeline: {name} (Clusters={use_cluster}) ===")

    train_ds = ArabicDiacritizationDataset(data_config['train_data'], data_config['c2i'], data_config['d2i'], data_config['w2i'], data_config['w2c'])
    val_ds = ArabicDiacritizationDataset(data_config['val_data'], data_config['c2i'], data_config['d2i'], data_config['w2i'], data_config['w2c'])

    train_loader = DataLoader(train_ds, batch_size=hp['BS'], shuffle=True, collate_fn=collate_batch)
    val_loader = DataLoader(val_ds, batch_size=hp['BS'], shuffle=False, collate_fn=collate_batch)

    feature_extractor = FeatureExtractor(
        data_config['CV_SIZE'], data_config['WV_SIZE'], hp['NUM_CLUSTERS'],
        hp['C_EMB'], hp['W_EMB'], hp['CL_EMB'],
        use_word, use_cluster, use_flag
    )

    # Instantiate BiRNN instead of BiLSTM
    model = DiacritizerBiRNN(feature_extractor, hp['HIDDEN'], hp['LAYERS'], data_config['DV_SIZE'], hp['DROP']).to(DEVICE)

    criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)
    optimizer = optim.Adam(model.parameters(), lr=hp['LR'])

    return train_model(model, train_loader, val_loader, criterion, optimizer, hp['EPOCHS'], name)

# --- 9. MAIN ---
def load_data(path):
    try:
        with open(path, 'r', encoding='utf-8') as f: sent = f.read().splitlines()
    except: sent = ["ذهب الولد", "بسم الله"] * 20
    proc, ch, di, wo, raw_wo_sent = [], [], [], [], []
    for s in sent:
        cs = clean_sentence(s)
        if not cs: continue
        c, d = separate_chars_and_diacritics(cs)
        if c:
            proc.append((c, d))
            ch.extend(c); di.extend(d);
            words = "".join(c).split(SPACE_CHAR)
            wo.extend(words)
            raw_wo_sent.append(words)
    return proc, ch, di, wo, raw_wo_sent

if __name__ == '__main__':
    # --- 1. HYPERPARAMETERS ---
    HP = {
        'C_EMB': 128,    # Character Embedding Dim
        'W_EMB': 256,    # Word Embedding Dim
        'CL_EMB': 32,    # Cluster Embedding Dim
        'HIDDEN': 256,   # BiRNN Hidden Size
        'LAYERS': 3,     # Number of Layers
        'DROP': 0.5,     # Dropout Rate
        'LR': 0.001,     # Learning Rate
        'EPOCHS': 25,    # Epochs per run
        'BS': 64,        # Batch Size
        'NUM_CLUSTERS': 50 # K-Means Clusters
    }

    # --- 2. LOAD DATA ---
    print(">>> Loading Data...")
    tr_data, t_c, t_d, t_w, t_raw_sentences = load_data('/kaggle/input/dataset-title/train.txt')
    va_data, v_c, v_d, v_w, _ = load_data('/kaggle/input/dataset-title/val.txt')

    # --- 3. BUILD VOCABULARIES ---
    print(">>> Building Vocabularies...")
    c2i, d2i, w2i, CV, DV, WV = create_vocabs(t_c+v_c, t_d+v_d, t_w+v_w)

    # --- 4. TRAIN WORD2VEC + K-MEANS ---
    print(">>> Generating Semantic Clusters (W2V + K-Means)...")
    word_to_cluster_map = train_w2v_kmeans_clusters(
        t_raw_sentences,
        num_clusters=HP['NUM_CLUSTERS'],
        vector_size=100
    )

    CONFIG = {
        'train_data': tr_data, 'val_data': va_data,
        'c2i': c2i, 'd2i': d2i, 'w2i': w2i, 'w2c': word_to_cluster_map,
        'CV_SIZE': CV, 'DV_SIZE': DV, 'WV_SIZE': WV
    }

    # --- 5. DEFINE THE 8 COMBINATIONS ---
    # Format: (Name, Use_Word, Use_Cluster, Use_Flag)
    experiments = [
        ("8_FULL_MODEL",          True,  True,  True),
        ("6_Char_Word_Flag",      True,  False, True),
        ("4_Char_Cluster_Flag",   False, True,  True),
        ("3_Char_Cluster",        False, True,  False),
        ("5_Char_Word",           True,  False, False),

        ("7_Char_Word_Cluster",   True,  True,  False),
        ("2_Char_Flag",           False, False, True),
        ("1_Char_Only",           False, False, False),
    ]

    results = {}

    print(f"\n{'='*60}")
    print(f"   STARTING EXTENSIVE ABLATION STUDY (8 RUNS)   ")
    print(f"{'='*60}\n")

    for name, use_word, use_cluster, use_flag in experiments:
        print(f"--- Running: {name} ---")
        acc = run_pipeline(name, use_word, use_cluster, use_flag, CONFIG, HP)
        results[name] = acc
        print(f"--- Result: {acc:.2f}% ---\n")

    # --- 6. FINAL SUMMARY TABLE ---
    print(f"\n{'='*45}")
    print(f"| {'Model Name':<25} | {'Val Acc':<10} |")
    print(f"|{'-'*27}|{'-'*12}|")

    # Sort results by accuracy (Best on top)
    sorted_results = sorted(results.items(), key=lambda item: item[1], reverse=True)

    for name, acc in sorted_results:
        print(f"| {name:<25} | {acc:.2f}%      |")
    print(f"{'='*45}")